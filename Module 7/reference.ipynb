{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5BLpBB_cv8y"
      },
      "source": [
        "**Logistic Regression:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQA2GUq7cPXd"
      },
      "source": [
        "![Picture4.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAkMAAABLCAMAAABEK3PtAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAzUExURQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKMFRskAAAAQdFJOUwATIi88SlhndoWWqLjL3O5QTd6UAAAACXBIWXMAABcRAAAXEQHKJvM/AAAEFklEQVR4Xu3ci3KjIBQGYEGDiAi8/9Pu4ZJNE3Cjkm3F/t/MjiaTpo3+Hi6S7QAAAAAAAAAAAAAAAOATuNJpD+AIppxDhuA4Ju1skCGocJt5J5EhqIQMQS1kCGohQ1ALGYJayBDUQoagFjIEtZAhqIUMQS1kCGohQ1CpN84NaR9gP3eHGF2dcgtLuwBHjG5GpwVqCKfCP4CD+tCQKSfSYwCAb3YfMzHr+rADsJNK3WnMBcJRPBYiKkOYw4GDVBiSjStlSKZJwkSmpwG+oELEu86slCFkCDbwhUigNwQVfCFa9vaGUl36pPTO0CJ14F5HOu+flN4ZWkSFCIMyqKPckvYy6FP/OH8Kzj/9+4/5RWQoGtIBIPUlW/u3oa3fbLjXLZxNeyeGOeq3pFs4bZh1c3yiinHWv9uNIrRh4dbwkd/5n/1ohqy/Gp2z514FN8e/Tzvzib9TxM7DLQ+HLJS50Y1p78R+MkM8tI/a3eLDs7JhaQy17B8ZfDDnKIrM5MtHSxlSLdwNP5whJqtbauF/dSOrl/qPdQhn/4F1aNCelTJkGugO+cWwaW8fSe1Q2q0ytrGKkm39fwF9B9xvWWiyqHhlIRB0xKdSeSlkiNFrb4szV1yaI8xCA4z04Cu9M1j+gLZAxZ7wQxhfPTzO/hjaKp8l/2gqRM86Uay9hQzd3KhGeq+T9xiP4Hq4H6QXbzL0mC6IDUNv2/hWCY2iXjptqxlKK/umeHhkIS1qpVksZIiGhL5PvZy9y3jQoQy9YLaNC4zbHS1uXI1lwpKIbjbhuSfjypxPIUM6ljF90cm5HRkSdNGaQvVmi22joV/2fAlP+ygIO/kNL5ShnnqSzx/b96G++NL6pQL36zPEtBv9sCY/C2GE0i2nr9PUAu/IejjhWoaqMuU3kpiZX8cRqxnq042oqy403ZyhJSQlXJ3PlJ8/Y6oQrnOhT7pnok9ShoRlkn6IF869XhgvN2Z5Wza6yW946QK8gq0ZknHkZbJL+X71FXoMp1Ie1q/3qSlDjFpuQVHSIQJPlG++y33kPENzfN1lVwm+ZGjtBm363lHDh2FOI2v59AnWMyTcLKkJGpwc85sjcVRfnpTLM5RWKa8tVm7exgwJqjP8ppxutRqLNKzvY7vy3uCMpXPuN6na0LGJO/RkCA8du8LhyDKUvndzdBr4/Da2ZZNPlJ6avZBoFBWyM9itczR0YMJPpA35myF6s3Ag6CWFgVaWIX9vv6OOVbPX3zsbM9T6sHQOZTXYeibT/f3HfMA9Qzca1fve32Do3fJBf5ahUQv69Uv+yqv4JRn6Rnl/6Oo2ZsiPdEl/0ZEFVOBUZQsVJssQt4Z3XJamqeFX+zuyfY1RlqGup7CZKV8tA1CWZwgAAAAAAAAAAAAAAGC7rvsDkSM8e6OA8XgAAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0dxwE-Ubsdf"
      },
      "source": [
        "Y_hat --> predicted value\n",
        "\n",
        "X --> Input Variable\n",
        "\n",
        "w --> weight\n",
        "\n",
        "b --> bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8OJXGuPvDt2"
      },
      "source": [
        "**Gradient Descent:**\n",
        "\n",
        "Gradient Descent is an optimization algorithm used for minimizing the loss function in various machine learning algorithms. It is used for updating the parameters of the learning model.\n",
        "\n",
        "w  =  w - α*dw\n",
        "\n",
        "b  =  b - α*db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSAfYP7WmECB"
      },
      "source": [
        "**Learning Rate:**\n",
        "\n",
        "Learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8AfHQz9cXRF"
      },
      "source": [
        "**Derivatives:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLUw3M-WcCwv"
      },
      "source": [
        "![Picture3.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxOpuBj_SqV-"
      },
      "source": [
        "Importing the Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdjRDi8wlgX6"
      },
      "outputs": [],
      "source": [
        "# importing numpy library\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTA-BwebLjdc"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcqEjNwtKHPq"
      },
      "outputs": [],
      "source": [
        "class Logistic_Regression():\n",
        "\n",
        "\n",
        "  # declaring learning rate & number of iterations (Hyperparametes)\n",
        "  def __init__(self, learning_rate, no_of_iterations):\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.no_of_iterations = no_of_iterations\n",
        "\n",
        "\n",
        "\n",
        "  # fit function to train the model with dataset\n",
        "  def fit(self, X, Y):\n",
        "\n",
        "    # number of data points in the dataset (number of rows)  -->  m\n",
        "    # number of input features in the dataset (number of columns)  --> n\n",
        "    self.m, self.n = X.shape\n",
        "\n",
        "\n",
        "    #initiating weight & bias value\n",
        "\n",
        "    self.w = np.zeros(self.n)\n",
        "    \n",
        "    self.b = 0\n",
        "\n",
        "    self.X = X\n",
        "\n",
        "    self.Y = Y\n",
        "\n",
        "\n",
        "    # implementing Gradient Descent for Optimization\n",
        "\n",
        "    for i in range(self.no_of_iterations):\n",
        "      self.update_weights()\n",
        "\n",
        "\n",
        "\n",
        "  def update_weights(self):\n",
        "\n",
        "    # Y_hat formula (sigmoid function)\n",
        "\n",
        "    Y_hat = 1 / (1 + np.exp( - (self.X.dot(self.w) + self.b ) ))    \n",
        "\n",
        "\n",
        "    # derivaties\n",
        "\n",
        "    dw = (1/self.m)*np.dot(self.X.T, (Y_hat - self.Y))\n",
        "\n",
        "    db = (1/self.m)*np.sum(Y_hat - self.Y)\n",
        "\n",
        "\n",
        "    # updating the weights & bias using gradient descent\n",
        "\n",
        "    self.w = self.w - self.learning_rate * dw\n",
        "\n",
        "    self.b = self.b - self.learning_rate * db\n",
        "\n",
        "\n",
        "  # Sigmoid Equation & Decision Boundary\n",
        "\n",
        "  def predict(self, X):\n",
        "\n",
        "    Y_pred = 1 / (1 + np.exp( - (X.dot(self.w) + self.b ) )) \n",
        "    Y_pred = np.where( Y_pred > 0.5, 1, 0)\n",
        "    return Y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcqEjNwtKHPq"
      },
      "outputs": [],
      "source": [
        "class Logistic_Regression():\n",
        "\n",
        "\n",
        "  # declaring learning rate & number of iterations (Hyperparametes)\n",
        "  def __init__(self, learning_rate, no_of_iterations):\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.no_of_iterations = no_of_iterations\n",
        "\n",
        "\n",
        "\n",
        "  # fit function to train the model with dataset\n",
        "  def fit(self, X, Y):\n",
        "\n",
        "    # number of data points in the dataset (number of rows)  -->  m\n",
        "    # number of input features in the dataset (number of columns)  --> n\n",
        "    self.m, self.n = X.shape\n",
        "\n",
        "\n",
        "    #initiating weight & bias value\n",
        "\n",
        "    self.w = np.zeros(self.n)\n",
        "    \n",
        "    self.b = 0\n",
        "\n",
        "    self.X = X\n",
        "\n",
        "    self.Y = Y\n",
        "\n",
        "\n",
        "    # implementing Gradient Descent for Optimization\n",
        "\n",
        "    for i in range(self.no_of_iterations):\n",
        "      self.update_weights()\n",
        "\n",
        "\n",
        "\n",
        "  def update_weights(self):\n",
        "\n",
        "    # Y_hat formula (sigmoid function)\n",
        "\n",
        "    Y_hat = 1 / (1 + np.exp( - (self.X.dot(self.w) + self.b ) ))    \n",
        "\n",
        "\n",
        "    # derivaties\n",
        "\n",
        "    dw = (1/self.m)*np.dot(self.X.T, (Y_hat - self.Y))\n",
        "\n",
        "    db = (1/self.m)*np.sum(Y_hat - self.Y)\n",
        "\n",
        "\n",
        "    # updating the weights & bias using gradient descent\n",
        "\n",
        "    self.w = self.w - self.learning_rate * dw\n",
        "\n",
        "    self.b = self.b - self.learning_rate * db\n",
        "\n",
        "\n",
        "  # Sigmoid Equation & Decision Boundary\n",
        "\n",
        "  def predict(self, X):\n",
        "\n",
        "    Y_pred = 1 / (1 + np.exp( - (X.dot(self.w) + self.b ) )) \n",
        "    Y_pred = np.where( Y_pred > 0.5, 1, 0)\n",
        "    return Y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htiH07T_WL-Y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "7.2.5. Building Logistic Regression from scratch in Python.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
